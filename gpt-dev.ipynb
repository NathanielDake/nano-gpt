{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-18 06:50:22--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  4.95MB/s    in 0.2s    \n",
      "\n",
      "2023-01-18 06:50:22 (4.95 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  1115394 \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print('Total number of characters: ', len(text), '\\n')\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Get the unique characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "Take the raw text as a string and convert it to some sequence of integers according to a vocabularly of possible elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from characters to integers. This is a simple tokenizer that yields a small codebook.\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]           # encoder: takes in a string, outputs a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # This is what the 100 characters we looked at earlier will look like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # First 90% is train, rest is val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Training Transformers\n",
    "* We never feed the entire text into a transformer all at once. That would be very computationally expensive.\n",
    "* When we actually train the transformer on these datasets, we only work with chunks of the dataset. We will sample random little chunks out of the training set and train at chunks at a time. These chunks will have some maximum length. This is known as the block size, or context length.\n",
    "* Below we see the first 9 characters in the train data set. When we sample a sequence of 9 characters like the one below, we must remember that this actually has _multiple_ examples packed into it. That is because all of these characters _follow_ eachother. \n",
    "* So, when plugging it into a transformer, we will actually simultaneously train it to make predictions at every single one of these positions. This means for a chunk of `9` characters we have `8` examples packed in there.\n",
    "* For example, in the context of `18`, `47` likely comes next. In the context of `18, 47`, `56` likely comes next. And so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# x and y are simply off by 1 (shifted) versions of eachother\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f'When input is {context} the target is: {target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that here we train on examples with context between length 1, all the way up to a context with the length of the block size. We train on that not simply for efficiency, but also in order to make the transformer network be used to seeing contexts all the way from as little as 1, all the way to block size. We would like the transformer to be used to seeing everything in between. This will be incredibly useful during _inference_ because while we are sampling, we can start with as little as one character of context. It can then predict everything up to block size, and at that point we will need to start truncating, because the transformer will never receive more than block size input when it's predicting the transformer. \n",
    "\n",
    "At this point we have looked at the \"**time**\" dimension of the data we will be feeding into the transformer. There is one more dimension that we are going to care about and that is the **batch** dimension. The idea of batches is done entirely for efficiency so we can keep our gpus busy :). Note though that each chunk will be processed completely independently, they do not talk to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # How many indepedent sequences will we process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "\n",
    "    # Get `batch_size` random integers in the range of 0 to len(data) - block size\n",
    "    # Must subtract block size to ensure any example has a full context window\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "\n",
    "    # For each integer, that marks the start of an example. Index into data to grab\n",
    "    # that integer up to integer + blocksize\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    \n",
    "    # Do the same for y, but just shift by 1 (remember this is an autoregressive model)\n",
    "    y = torch.stack([data[i + 1: i + 1 + block_size] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f'Inputs:\\n{xb.shape}\\n{xb}')\n",
    "print(f'Targets:\\n{yb.shape}\\n{yb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "        [25, 17, 27, 10,  0, 21,  1, 54]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our `(4, 8)` tensor above, we can see that we actually have `32` examples! These are all completely independent (as far as the transformer is concerned). We can see all 32 independent examples printed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, 0:t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this `x` will be fed into the transformer. The transformer will simultaneously process all of the examples (independently) and look up the correct integers to predict in every one of these positions in the tensor `y`. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simplest Implementation Possible: Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Our input into the transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are predicting what comes next based on just the individual identity of a single token. For this basic model the token are not talking to eachother and they are not seeing any context, they are just seeing themselves. For instance, `xb[0][0] = 5`. Given token `5` you can actually make decent predictions just by knowing that you are token `5`. \n",
    "\n",
    "A note on the embedding table below: In our `xb` example above, the first entry `xb[0][0]` is `24`. In that case `24` will be based in and will pluck out the 24th row of the embedding table. Also, note that after using our embedding table our `xb` will go from being `(4, 8)` to `(4, 8, 65)`. In other words, _each entry_ of `xb` will effectively be _mapped_ to a `65` dimensional \"embedding\". This embedding is really just a vector holding the logits associated with the next character in the sequence we are trying to predict.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) \n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \"\"\"Bigram language model class\n",
    "    \n",
    "    Notes:\n",
    "    - B: batch, T: time, C: channel. In this case: B: 4, T: 8, C: 65 (vocab size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Each token directly reads off the \"predicted\" logits from a lookup table for what token comes next in the sequence\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # Pluck out rows, arange them in the shape (B, T, C), and interpret them as the logits, which are effectively\n",
    "        # the scores for which character comes next in the sequence. \n",
    "        logits = self.token_embedding_table(idx) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Cross entropy expects (B*T, C). In other words, it wants a 2 dimensional tensor as input where each row \n",
    "            # holds a prediction (logits) and it can be matched up with the correct targets. We can think of this as\n",
    "            # \"stretching out\" our array\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets) # Here we are passing in target indices, not one hot encoded probabilities\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        # idx is (B, T) array of indices in the current context. E.g. this is xb.\n",
    "        # This function is meant to take (B, T) and generate subsequent tokens, making it \n",
    "        # (B, T+1), then (B, T+2), and so on, up to T+max_new_tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, _ = self.forward(idx)\n",
    "\n",
    "            # focus only on the last time step (grab last element of time dimension, predictions for what comes next)\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "\n",
    "            # Sample from distribution. Get a single prediction for what comes next for each batch dimension\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "\n",
    "# Remember, nn.Module as a method __call__ that points to forward, so this is a forward pass\n",
    "logits, loss = m(xb, yb) \n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have 65 vocabulary elements and right now we are effectively predicting them at random, we can expect our loss to roughly be: `-ln(1/65) = 4.17`. Our initial loss is 4.87, meaning our predictions aren't entirely _diffuse_ and they have a bit of _entropy_, hence we are guessing wrong. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets give `generate` a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sample_input = torch.zeros((1, 1), dtype=torch.long) \n",
    "x_sample_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 101])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 15, 54, 64, 50, 37, 39, 31,  1, 15, 41, 64, 42, 43, 15,  5, 39, 17,\n",
       "         20, 37,  6, 37, 64, 56, 64, 28, 19, 22, 33, 12, 37, 43, 46, 27, 24, 47,\n",
       "          0, 15, 48,  0, 15, 64, 53, 36,  3, 49, 33, 58, 31, 55, 13, 33, 33,  5,\n",
       "         64, 28, 63, 34, 22, 47, 19,  1, 20, 32, 53, 57, 58, 58, 62, 11, 39,  2,\n",
       "         33, 34, 64, 63, 13, 52, 57, 64,  7, 47, 51, 57, 64, 28, 13, 49, 44, 63,\n",
       "         39, 11, 10, 45, 33, 11, 30, 32, 26, 14, 19]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337) \n",
    "\n",
    "generated_output = m.generate(x_sample_input, max_new_tokens=100)\n",
    "print(generated_output.shape)\n",
    "generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "generated_output = generated_output[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCpzlYaS CczdeC'aEHY,YzrzPGJU?YehOLi\\nCj\\nCzoX$kUtSqAUU'zPyVJiG HTosttx;a!UVzyAnsz-imszPAkfya;:gU;RTNBG\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make human readable\n",
    "decode(generated_output)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes - this prediction is awful! But in fairness we just passed it a newline character and our model hasn't been trained at all haha. Not that the way in which we are writing the `generate` function we are not using _any_ of the history when making a prediction-we only use the current character! It is written this way so that when we start working with more advanced models we can effectively keep the `generate` function fixed. So this way we are providing all context to the model and the model gets to decide how much (if any) it wants to use to make it's predictions.\n",
    "\n",
    "### 1.1 Training the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4315860271453857\n"
     ]
    }
   ],
   "source": [
    "# Create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) # For very small networks we can get away with 1e-3 (larger nets would need 1e-4)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IARI he umin,\n",
      "YORISer Clco? cas tht magic m bou? s-efuril win d fiouby fon, asheche MELor? as.\n",
      "MED theean G sthith widangushenen, u athoonchar\n",
      "\n",
      "Aieavece I bu pls thayolste\n",
      "Thr ld wen tchty' lle, toug ouites 'Wout ath y se tharere gn.\n",
      "YCH:\n",
      "Whasonong mpprrde fofan'shano,\n",
      "Q! by hinomoor, heatis, trmbuty, wofron o,\n",
      "\n",
      "\n",
      "Wivit t,\n",
      "\n",
      "An.\n",
      "BEDUShea\n",
      "IDokliseathime sethen s winoirepe g,\n",
      "I borevo ncay! h cowhot'literdicow je wier tinthaloug t s spuer emmom'd thatheny-taweallove waby angeal tethexperd mes thacot\n"
     ]
    }
   ],
   "source": [
    "generated_output = m.generate(x_sample_input, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(decode(generated_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Few reminders about our training process!\n",
    "First just recall what a single entry of our inputs, mini batch `xb`, and their targets, `yb`, look like. `yb` is simply `xb` shifted by `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50, 53, 59, 57,  1, 46, 53, 53])\n",
      "tensor([53, 59, 57,  1, 46, 53, 53, 42])\n"
     ]
    }
   ],
   "source": [
    "print(xb[0])\n",
    "print(yb[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, recall what is happening during our training process. We specifically: \n",
    "1. Get a batch of 32 examples. Remember an example is currently `8` consecutive characters. For each of these characters we know the target (the subsequent character). So, for each of the 8 characters we can \"generate\" a prediction by keying into the \"embedding\" table\n",
    "2. So, for our batch of `32` examples we have `8` examples for each (I know, 'example' is overloaded here). So, that means we really have a total of `256` examples associated with a batch.\n",
    "3. We generate predictions for all `256` examples and compare them with the true target (remember, the target is simply the token (it's index specifically) that follows that follows our current token).\n",
    "4. We now have `256` predictions (each prediction is a `65` dimensional array of logits) and targets. We compute the loss on these set of predictions and targets. \n",
    "5. We then call `loss.backward()` which computes the gradient and updates our weights accordingly. \n",
    "\n",
    "#### Note on data\n",
    "I understand _why_ the code was written this way. However, it could have been easier to reason about if it was converted into a _tabular_ form and stored in a pandas dataframe. Or is it just that that is what I am used to? \n",
    "\n",
    "#### Note on embeddings\n",
    "I really don't like calling this an embedding, although it is one. The reason is that is doesn't conform with the usual use of the term embedding. Here we are taking our _objects_, the tokens in our vocabulary, and coming up with a `65` dimensional representation of them. Each dimension in this representation space corresponds to one of the tokens. The _value_ associated with each dimension for a particular tokens embeddings represents a sense of _how likely_ this token is to be followed by the token of the associated embedding dimension. \n",
    "\n",
    "Now, as we talk through this, calling this an embedding is really not that crazy. We have taken objects, tokens, and embedded them in a continuous space ($\\mathbb{R}^{65}$). Say we then have two tokens that have _similar_ embeddings (close by in euclidean space). The interpretation here is that these two tokens tend to be followed by other similar tokens!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformer\n",
    "### 2.1 Mathematical Trick of Self Attention\n",
    "We currently have `8` tokens in a _batch_ and they currently are _not_ talking to each other. We would like for them to talk to each other. We would like to couple them. \n",
    "\n",
    "We want to couple them in a very specific way. The token in the 5th location should not communicate with tokens in the 6th, 7th and 8th location because those are **future** tokens in the sequence. The token in the 5th location should only talk to the tokens in the 4th, 3rd, 2nd and 1st. So information only flows from the previous context to the current time step. We cannot get any information from the future because we are about to try to predict the future. \n",
    "\n",
    "What is the easiest way that we can go about this? The easiest way would be: if we are at the 5th token and we would like to communicate with our past, the simplest thing we can do would be an _average_ of all the preceeding elements. So we can take information from the _channels_ at the current (5th) step, as well as the 4th, 3rd, 2nd and 1st step, and then average those up. Then that would be a feature vector that summarizes the current step (5th) in the context of it's history. \n",
    "\n",
    "Now, of course simply doing a sum or average is an _extremely weak_ form of interaction. This communication is extremely **lossy**. We have lost a ton of information about the spatial arangement of those tokens. However, that is okay for now. We will see how we can bring that information back later!\n",
    "\n",
    "For now we would like to calculate:\n",
    "* For every single batch element independently \n",
    "* For every $t$th token in that sequence\n",
    "* We would like to calculate the average of all of the vectors in all of the previous tokens and also at this token. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2 # Batch, Time, Channels\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: No vectorization\n",
    "\n",
    "# We want x[b, t] = mean_{i <= t} x[b, i]\n",
    "xbow = torch.zeros((B, T, C))  # Calling this x bag of words (since bow is a term used when averaging up things)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, 0:t + 1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1490, -0.3199])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0:3].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a single example in the batch\n",
    "wei @ x[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For all examples in the batch (pytorch is smart with broadcasting)\n",
    "wei @ x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch will see that these dimensions are not the same so it will create a batch dimension and broadcast (copy) the `(T, T)` matrix, meaning it will be of shape `(B, T, T)`, and hence then be multiplied by a `(B, T, C)`. So, for a given batch, we will have a `(T, T)` matrix multiplied by a `(T, C)` matrix. This will result in a `(T, C)` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2: Vectorized \n",
    "\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x  # (T, T) @ (B, T, C) ----> Broadcast ----> (B, T, T) @ (B, T, C) ----> (B, T, C)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the trick is that we will use a _batched matrix multiply_ to perform a _weighted aggregation_, where the weights are specified in this `(T, T)` array. We are effectively doing weighted sums, where the token at the $t$th dimension only gets information from the tokens preceeding it. \n",
    "\n",
    "We can finally rewrite this in one more way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Use Softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))                       # Currently set by us to be 0\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Tokens from the past cannot communicate with future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x                                 # Aggregation \n",
    "\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that we will use this form in **self attention** is that the weights begin as `0`. You can think of this as an **interaction strength** (i.e. an **affinity**). It is telling us _how much of each token from the past do we want to aggregate (i.e. average up)_. \n",
    "\n",
    "So in the line:\n",
    "```\n",
    "wei = torch.zeros((T, T))       \n",
    "```\n",
    "\n",
    "It is currently just set by us to be `0`s. But these affinities between the tokens are not just going to be constant at `0`. They are going to be _data dependent_. These tokens are going to start looking at eachother. And some tokens will find other tokens more or less interesting. Depending on what their values are, they are going to find each other interesting to different amounts (affinities). \n",
    "\n",
    "So, the TLDR is: \n",
    "> You can do **weighted aggregations** of your past elements by using matrix multiplication of a lower triangular fashion. The elements in the lower triangular part are telling you how much of each element \"fuses\" into this position. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "Different tokens will find other tokens more or less interesting, and we want that to be _data dependent_. For instance, say the current token is a vowel. Then maybe it is specifically interested in the consonants it's past and it would like that information to flow to it. \n",
    "\n",
    "So, we want to gather information from the past and do it in a data dependent way. This is the problem that **self attention** solves! \n",
    "\n",
    "Self attention solves this as follows:\n",
    "1. Every single node (token), at each position, will emit two vectors: a **query** and a **key**. \n",
    "  * The query vector is: what am I looking for?\n",
    "  * The key vector is: what do I contain?\n",
    "2. The way that we get _affinities_ between the tokens in a sequence is we do a **dot product** between the keys and the queries. So our query dot products with all of the keys of the tokens, and that dot product now becomes `wei`, below. \n",
    "3. If the key and the query are aligned to a high degree they will interact to a high amount, and we will be able to learn more about that specific token as opposed to any other token in the sequence.\n",
    "\n",
    "Remember: So far we have been saying that we will aggregate information that could be useful for a token, say `5`, via taking the average of the previous tokens in the sequence (e.g. the 4th, 3rd, 2nd and 1st). This average vector will be a feature vector we can use. \n",
    "\n",
    "What we are saying now is that the average vector will be a _weighted average_, where the weights are based on how interesting/useful a certain token is with respect to our current token. This is a _learned_ process (i.e. our network learns which tokens a specific current token should focus on!)\n",
    "\n",
    "Also, we can think of `x` as private information to this token. So we could say: \"I'm the 5th token, I have some identity. My information is kept in vector `x`. And (for the purposes of this single head) here is what I am interested in, `query(x)`, here is what I have `key(x)`, and if you find me interesting here is what I will communicate to you `value(x)`. So `v` is the thing that gets aggregated for the purposes of this single head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: Self Attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C) # Batch, time, channels\n",
    "\n",
    "# A single head performing self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# All tokens, in all positions of the (B, T) arangement, in parallel\n",
    "# and independently, produce a key and a query. So NO communication\n",
    "# has happened yet\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# Communication comes NOW!\n",
    "# For every row, b in B, we will have a (T, T) matrix giving us the \n",
    "# affinities\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, 16) @ (B, 16, T) ---> (B, T, T) \n",
    "\n",
    "# This is the same as version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # Tokens from the past cannot communicate with future\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) \n",
    "out = wei @ v       # Aggregation \n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the row below. This was the 8th token. The 8th token knows what **content** it has, and it knows what **position** it is in. Now, based on that, the 8th token creates a query that says: \"hey, I'm looking for this kind of stuff - I'm a vowel, in the 8th position, I'm looking for any consonants up to position 4...\". Then all of the nodes get to emit keys. Maybe one of the channels could be: \"I am a consonant and I am in a position up to 4\". Then that key would have a high number in that specific channel. That is how the query and the key when they dot product they can find each other and create a high affinity. \n",
    "\n",
    "If they have a high affinity then via the softmax it will end up aggregating a lot of its information into the 8th tokens vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0][-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every single batch element will contain different weights/affinities! This is because every single batch element will contain different tokens! \n",
    "\n",
    "### Notes\n",
    "1. Attention is a **communication mechanism**. \n",
    "    * Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. \n",
    "    * Put another way: every node has some vector of information and it gets to aggregate information, via a weighted sum, of all the nodes that point to it. This is done in a data dependent manner. \n",
    "    * In our scenario the directed graph is very simple. The first node in a single example of our batch (i.e. 8 tokens) will just point to itself. The second node is pointed to by the first node and itself. The third nodes is point to by the 1st and 2nd node, and itself. All the way up to the 8th node which is pointed to by the first 7 nodes and itself. In this way our `tril` matrix effectively acts as a DAG in matrix form. \n",
    "    * In principle attention can be applied to _any arbitrary directed graph_! It is simply a communication mechanism between the nodes. \n",
    "2. There is no notion of **space**. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. \n",
    "3. Each example across the batch dimensions is of course processed completely independently and they never \"talk\" to each other. \n",
    "4. In the case of language modeling we have a constraint that future tokens will never talk to past tokens. In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate (you may want this to be the case in sentiment analysis). This block we have implemented above is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "5. \"self-attention\" just means that the keys and values are produced from the same source as queries. In principle, however, attention is much more general than that. For instance, the queries could be based on `x`, but the keys and values could come from an entirely separate source. In \"cross-attention\", the queries still get produced from `x`, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "6. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 63, 52, 39, 51, 43, 47, 57]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('mynameis')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Optimizations\n",
    "#### Residual Connections\n",
    "The supervision that we experience from the loss will hop through every node all the way to the input, and also fork off into the residual blocks. We can think of this as a \"gradient super highway\" that goes all the way to the input, unimpeded. The blocks are initialized in such a way that in the beginning they contribute very little to the loss. But over time they \"come online\" and start to contribute. However, at initialization you can go directly from the loss to the input, the gradient just flows unimpeded, and over time the blocks start to kick in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "520c05642913265be95fb7a9b4141d22970002e3d25cc40aba527028a69f6911"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
